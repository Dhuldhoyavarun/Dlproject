{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcVvmBwVK_f3",
        "outputId": "c6c265f8-cea3-48b8-afbc-e5f6ab807a09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3iHqqTULUs9"
      },
      "source": [
        "#unzip and add to drive\r\n",
        "#!unzip \"/content/drive/MyDrive/dlproject/Complex-YOLO.zip\" -d \"/content/drive/MyDrive/dlproject\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI-dZpVzWGMW"
      },
      "source": [
        "from __future__ import division\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "from scipy import misc\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as data\r\n",
        "import math\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ldOy2F6WTmC"
      },
      "source": [
        "#classes in KITTI DATASET\r\n",
        "class_list = ['Car', 'Van' , 'Truck' , 'Pedestrian' , 'Person_sitting' , 'Cyclist' , 'Tram' ]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiur-BCS2coZ"
      },
      "source": [
        "file = open(\"/content/drive/MyDrive/dlproject/training/train.txt\",\"w\")\r\n",
        "\r\n",
        "\r\n",
        "for i in range(6000):\r\n",
        "    file_i = str(i).zfill(6)\r\n",
        "    file.write(file_i + \"\\n\")\r\n",
        "   \r\n",
        "\r\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB8qu-1pVVW7"
      },
      "source": [
        "# **Utils**\r\n",
        "\r\n",
        "Helper functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKUXEABUVSiw"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# classes\r\n",
        "class_list = ['Car', 'Van' , 'Truck' , 'Pedestrian' , 'Person_sitting' , 'Cyclist' , 'Tram' ]\r\n",
        "\r\n",
        "\r\n",
        "bc={}\r\n",
        "bc['minX'] = 0; bc['maxX'] = 80; bc['minY'] = -40; bc['maxY'] = 40\r\n",
        "bc['minZ'] =-2; bc['maxZ'] = 1.25\r\n",
        "\r\n",
        "\r\n",
        "def removePoints(PointCloud, BoundaryCond):\r\n",
        "    \r\n",
        "    # Boundary condition\r\n",
        "    minX = BoundaryCond['minX'] ; maxX = BoundaryCond['maxX']\r\n",
        "    minY = BoundaryCond['minY'] ; maxY = BoundaryCond['maxY']\r\n",
        "    minZ = BoundaryCond['minZ'] ; maxZ = BoundaryCond['maxZ']\r\n",
        "    \r\n",
        "    # Remove the point out of range x,y,z\r\n",
        "    mask = np.where((PointCloud[:, 0] >= minX) & (PointCloud[:, 0]<=maxX) & (PointCloud[:, 1] >= minY) & (PointCloud[:, 1]<=maxY) & (PointCloud[:, 2] >= minZ) & (PointCloud[:, 2]<=maxZ))\r\n",
        "    PointCloud = PointCloud[mask]\r\n",
        "\r\n",
        "    PointCloud[:,2] = PointCloud[:,2]+2\r\n",
        "    return PointCloud\r\n",
        "\r\n",
        "def makeBVFeature(PointCloud_, BoundaryCond, Discretization):\r\n",
        "    # 1024 x 1024 x 3\r\n",
        "    Height = 1024+1\r\n",
        "    Width = 1024+1\r\n",
        "\r\n",
        "    # Discretize Feature Map\r\n",
        "    PointCloud = np.copy(PointCloud_)\r\n",
        "    PointCloud[:,0] = np.int_(np.floor(PointCloud[:,0] / Discretization))\r\n",
        "    PointCloud[:,1] = np.int_(np.floor(PointCloud[:,1] / Discretization) + Width/2)\r\n",
        "    \r\n",
        "    # sort-3times\r\n",
        "    indices = np.lexsort((-PointCloud[:,2],PointCloud[:,1],PointCloud[:,0]))\r\n",
        "    PointCloud = PointCloud[indices]\r\n",
        "\r\n",
        "    # Height Map\r\n",
        "    heightMap = np.zeros((Height,Width))\r\n",
        "\r\n",
        "    _, indices = np.unique(PointCloud[:,0:2], axis=0, return_index=True)\r\n",
        "    PointCloud_frac = PointCloud[indices]\r\n",
        "    #some important problem is image coordinate is (y,x), not (x,y)\r\n",
        "    heightMap[np.int_(PointCloud_frac[:,0]), np.int_(PointCloud_frac[:,1])] = PointCloud_frac[:,2]\r\n",
        "\r\n",
        "\r\n",
        "    # Intensity Map & DensityMap\r\n",
        "    intensityMap = np.zeros((Height,Width))\r\n",
        "    densityMap = np.zeros((Height,Width))\r\n",
        "    \r\n",
        "    _, indices, counts = np.unique(PointCloud[:,0:2], axis = 0, return_index=True,return_counts = True)\r\n",
        "    PointCloud_top = PointCloud[indices]\r\n",
        "\r\n",
        "    normalizedCounts = np.minimum(1.0, np.log(counts + 1)/np.log(64))\r\n",
        "    \r\n",
        "    intensityMap[np.int_(PointCloud_top[:,0]), np.int_(PointCloud_top[:,1])] = PointCloud_top[:,3]\r\n",
        "    densityMap[np.int_(PointCloud_top[:,0]), np.int_(PointCloud_top[:,1])] = normalizedCounts\r\n",
        "    \"\"\"\r\n",
        "    plt.imshow(densityMap[:,:])\r\n",
        "    plt.pause(2)\r\n",
        "    plt.close()\r\n",
        "    plt.show()\r\n",
        "    plt.pause(2)\r\n",
        "    plt.close()\r\n",
        "    plt.show(block=False)\r\n",
        "    plt.pause(2)\r\n",
        "    plt.close()\r\n",
        "    plt.imshow(intensityMap[:,:])\r\n",
        "    plt.show(block=False)\r\n",
        "    plt.pause(2)\r\n",
        "    plt.close()\r\n",
        "    \"\"\"\r\n",
        "    RGB_Map = np.zeros((Height,Width,3))\r\n",
        "    RGB_Map[:,:,0] = densityMap      # r_map\r\n",
        "    RGB_Map[:,:,1] = heightMap       # g_map\r\n",
        "    RGB_Map[:,:,2] = intensityMap    # b_map\r\n",
        "    \r\n",
        "    save = np.zeros((512,1024,3))\r\n",
        "    save = RGB_Map[0:512,0:1024,:]\r\n",
        "    #misc.imsave('test_bv.png',save[::-1,::-1,:])\r\n",
        "    #misc.imsave('test_bv.png',save)   \r\n",
        "    return save\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_target(label_file,Tr):\r\n",
        "    target = np.zeros([50, 7], dtype=np.float32)\r\n",
        "    \r\n",
        "    with open(label_file,'r') as f:\r\n",
        "        lines = f.readlines()\r\n",
        "\r\n",
        "    num_obj = len(lines)\r\n",
        "    index=0\r\n",
        "    for j in range(num_obj):\r\n",
        "        obj = lines[j].strip().split(' ')\r\n",
        "        obj_class = obj[0].strip()\r\n",
        "        #print(obj)\r\n",
        "\r\n",
        "\r\n",
        "        if obj_class in class_list:\r\n",
        "             \r\n",
        "             t_lidar , box3d_corner = box3d_cam_to_velo(obj[8:], Tr)   # get target  3D object location x,y\r\n",
        "             location_x = t_lidar[0][0]          \r\n",
        "             location_y = t_lidar[0][1]\r\n",
        "             #print(t_lidar)             \r\n",
        "\r\n",
        "             if  (location_x>0) & (location_x<40) & (location_y>-40)  & (location_y<40) :\r\n",
        "                  #print(obj_class)\r\n",
        "                  target[index][2] = t_lidar[0][0]/40             # make sure target inside the covering area (0,1)\r\n",
        "                  target[index][1] = (t_lidar[0][1]+40)/80             ## we should put this in [0,1] ,so divide max_size  80 m\r\n",
        "\r\n",
        "\r\n",
        "                  obj_width  = obj[9].strip()\r\n",
        "                  obj_length = obj[10].strip()\r\n",
        "                  target[index][3]=float(obj_width)/80\r\n",
        "                  target[index][4]=float(obj_length)/40     # get target width ,length\r\n",
        "\r\n",
        "\r\n",
        "                  obj_alpha = obj[3].strip()            # get target Observation angle of object, ranging [-pi..pi]\r\n",
        "                  target[index][5]=math.sin(float(obj_alpha))    #complex YOLO   Im\r\n",
        "                  target[index][6]=math.cos(float(obj_alpha))    #complex YOLO   Re\r\n",
        "\r\n",
        "                  #print(np.arctan2(target[0][4],target[0][5]))\r\n",
        "    \r\n",
        "                  for i in range(len(class_list)):\r\n",
        "                       if obj_class == class_list[i]:     # get target class\r\n",
        "                              target[index][0]=i\r\n",
        "                  index=index+1\r\n",
        "\r\n",
        "    return target\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def box3d_cam_to_velo(box3d, Tr):\r\n",
        "\r\n",
        "    def project_cam2velo(cam, Tr):\r\n",
        "        T = np.zeros([4, 4], dtype=np.float32)\r\n",
        "        T[:3, :] = Tr\r\n",
        "        T[3, 3] = 1\r\n",
        "        T_inv = np.linalg.inv(T)\r\n",
        "        lidar_loc_ = np.dot(T_inv, cam)\r\n",
        "        lidar_loc = lidar_loc_[:3]\r\n",
        "        return lidar_loc.reshape(1, 3)\r\n",
        "\r\n",
        "    def ry_to_rz(ry):\r\n",
        "        angle = -ry - np.pi / 2\r\n",
        "\r\n",
        "        if angle >= np.pi:\r\n",
        "            angle -= np.pi\r\n",
        "        if angle < -np.pi:\r\n",
        "            angle = 2*np.pi + angle\r\n",
        "\r\n",
        "        return angle\r\n",
        "\r\n",
        "    h,w,l,tx,ty,tz,ry = [float(i) for i in box3d]\r\n",
        "    cam = np.ones([4, 1])\r\n",
        "    cam[0] = tx\r\n",
        "    cam[1] = ty\r\n",
        "    cam[2] = tz\r\n",
        "    t_lidar = project_cam2velo(cam, Tr)\r\n",
        "\r\n",
        "    Box = np.array([[-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2],\r\n",
        "                    [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2],\r\n",
        "                    [0, 0, 0, 0, h, h, h, h]])\r\n",
        "\r\n",
        "    rz = ry_to_rz(ry)\r\n",
        "\r\n",
        "    rotMat = np.array([\r\n",
        "        [np.cos(rz), -np.sin(rz), 0.0],\r\n",
        "        [np.sin(rz), np.cos(rz), 0.0],\r\n",
        "        [0.0, 0.0, 1.0]])\r\n",
        "\r\n",
        "    velo_box = np.dot(rotMat, Box)\r\n",
        "\r\n",
        "    cornerPosInVelo = velo_box + np.tile(t_lidar, (8, 1)).T\r\n",
        "\r\n",
        "    box3d_corner = cornerPosInVelo.transpose()\r\n",
        "\r\n",
        "    return t_lidar , box3d_corner.astype(np.float32)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def load_kitti_calib(calib_file):\r\n",
        "    \"\"\"\r\n",
        "    load projection matrix\r\n",
        "    \"\"\"\r\n",
        "    with open(calib_file) as fi:\r\n",
        "        lines = fi.readlines()\r\n",
        "        assert (len(lines) == 8)\r\n",
        "\r\n",
        "    obj = lines[0].strip().split(' ')[1:]\r\n",
        "    P0 = np.array(obj, dtype=np.float32)\r\n",
        "    obj = lines[1].strip().split(' ')[1:]\r\n",
        "    P1 = np.array(obj, dtype=np.float32)\r\n",
        "    obj = lines[2].strip().split(' ')[1:]\r\n",
        "    P2 = np.array(obj, dtype=np.float32)\r\n",
        "    obj = lines[3].strip().split(' ')[1:]\r\n",
        "    P3 = np.array(obj, dtype=np.float32)\r\n",
        "    obj = lines[4].strip().split(' ')[1:]\r\n",
        "    R0 = np.array(obj, dtype=np.float32)\r\n",
        "    obj = lines[5].strip().split(' ')[1:]\r\n",
        "    Tr_velo_to_cam = np.array(obj, dtype=np.float32)\r\n",
        "    obj = lines[6].strip().split(' ')[1:]\r\n",
        "    Tr_imu_to_velo = np.array(obj, dtype=np.float32)\r\n",
        "\r\n",
        "    return {'P2': P2.reshape(3, 4),\r\n",
        "            'R0': R0.reshape(3, 3),\r\n",
        "            'Tr_velo2cam': Tr_velo_to_cam.reshape(3, 4)}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "anchors = [[1.08,1.19], [3.42,4.41], [6.63,11.38], [9.42,5.11], [16.62,10.52]]\r\n",
        "\r\n",
        "\r\n",
        "def bbox_iou(box1, box2, x1y1x2y2=True):\r\n",
        "    \"\"\"\r\n",
        "    Returns the IoU of two bounding boxes\r\n",
        "    \"\"\"\r\n",
        "    if not x1y1x2y2:\r\n",
        "        # Transform from center and width to exact coordinates\r\n",
        "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\r\n",
        "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\r\n",
        "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\r\n",
        "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\r\n",
        "    else:\r\n",
        "        # Get the coordinates of bounding boxes\r\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\r\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\r\n",
        "\r\n",
        "    # get the corrdinates of the intersection rectangle\r\n",
        "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\r\n",
        "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\r\n",
        "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\r\n",
        "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\r\n",
        "    # Intersection area\r\n",
        "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\r\n",
        "        inter_rect_y2 - inter_rect_y1 + 1, min=0\r\n",
        "    )\r\n",
        "    # Union Area\r\n",
        "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\r\n",
        "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\r\n",
        "\r\n",
        "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\r\n",
        "\r\n",
        "    return iou\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def convert2cpu(gpu_matrix):\r\n",
        "    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)\r\n",
        "\r\n",
        "\r\n",
        "def convert2cpu_long(gpu_matrix):\r\n",
        "    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIBXxNd7JApQ"
      },
      "source": [
        "# Kitty\r\n",
        "\r\n",
        "Load the KITTI dataset for training\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4EpAqqEVjaF"
      },
      "source": [
        "class KittiDataset(torch.utils.data.Dataset):\r\n",
        "\r\n",
        "    def __init__(self, root='/content/drive/MyDrive/dlproject',set='train',type='velodyne_train'):\r\n",
        "        self.type = type\r\n",
        "        self.root = root\r\n",
        "        self.data_path = os.path.join(root, 'training')\r\n",
        "        self.lidar_path = os.path.join(self.data_path, \"velodyne/\")\r\n",
        "        self.image_path = os.path.join(self.data_path, \"image_2/\")\r\n",
        "        self.calib_path = os.path.join(self.data_path, \"calib/\")\r\n",
        "        self.label_path = os.path.join(self.data_path, \"label_2/\")\r\n",
        "\r\n",
        "        with open(os.path.join(self.data_path, '%s.txt' % set)) as f:\r\n",
        "            self.file_list = f.read().splitlines()\r\n",
        "\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "\r\n",
        "        lidar_file = self.lidar_path + '/' + self.file_list[i] + '.bin'\r\n",
        "        calib_file = self.calib_path + '/' + self.file_list[i] + '.txt'\r\n",
        "        label_file = self.label_path + '/' + self.file_list[i] + '.txt'\r\n",
        "        image_file = self.image_path + '/' + self.file_list[i] + '.png'\r\n",
        "        #print(self.file_list[i])\r\n",
        "\r\n",
        "        if self.type == 'velodyne_train':\r\n",
        "\r\n",
        "            calib = load_kitti_calib(calib_file)\r\n",
        "\r\n",
        "            \r\n",
        "            target = get_target(label_file,calib['Tr_velo2cam'])\r\n",
        "            #print(target)\r\n",
        "            #print(self.file_list[i])\r\n",
        "            \r\n",
        "            ################################\r\n",
        "            # load point cloud data\r\n",
        "            a = np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 4)\r\n",
        "\r\n",
        "            b = removePoints(a,bc)\r\n",
        "\r\n",
        "            data = makeBVFeature(b, bc ,40/512)   # (512, 1024, 3)\r\n",
        "\r\n",
        "            return data , target\r\n",
        "\r\n",
        "        elif self.type == 'velodyne_test':\r\n",
        "            NotImplemented\r\n",
        "\r\n",
        "        else:\r\n",
        "            raise ValueError('the type invalid')\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.file_list)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChC-nOI5Vwgo"
      },
      "source": [
        "# Region_Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5H44_anVzeT"
      },
      "source": [
        "def build_targets(pred_boxes,pred_conf, pred_cls, target, anchors, num_anchors, num_classes, nH, nW, ignore_thres):\r\n",
        "    nB = target.size(0)\r\n",
        "    nA = num_anchors   #5\r\n",
        "    nC = num_classes   #8\r\n",
        "    mask = torch.zeros(nB,nA,nH,nW)\r\n",
        "    conf_mask  = torch.ones(nB, nA, nH, nW)\r\n",
        "    tx         = torch.zeros(nB, nA, nH, nW)\r\n",
        "    ty         = torch.zeros(nB, nA, nH, nW) \r\n",
        "    tw         = torch.zeros(nB, nA, nH, nW) \r\n",
        "    tl         = torch.zeros(nB, nA, nH, nW)\r\n",
        "    tim        = torch.zeros(nB, nA, nH, nW)\r\n",
        "    tre        = torch.zeros(nB, nA, nH, nW)\r\n",
        "    tconf      = torch.ByteTensor(nB, nA, nH, nW).fill_(0)\r\n",
        "    tcls       = torch.ByteTensor(nB, nA, nH, nW , nC).fill_(0)\r\n",
        "\r\n",
        "    nGT = 0\r\n",
        "    nCorrect = 0\r\n",
        "    for b in range(nB):\r\n",
        "        for t in range(target.shape[1]):\r\n",
        "            if target[b][t].sum() == 0:\r\n",
        "                continue\r\n",
        "\r\n",
        "            nGT += 1\r\n",
        "            # Convert to position relative to box\r\n",
        "            gx = target[b, t, 1] * nW\r\n",
        "            gy = target[b, t, 2] * nH\r\n",
        "            gw = target[b, t, 3] * nW\r\n",
        "            gl = target[b, t, 4] * nH\r\n",
        "            #gim = target[b][t][5]\r\n",
        "            #gre = target[b][t][6]\r\n",
        "\r\n",
        "            # Get grid box indices\r\n",
        "            gi = int(gx)\r\n",
        "            gj = int(gy)\r\n",
        "            # Get shape of gt box\r\n",
        "            gt_box = torch.FloatTensor(np.array([0, 0, gw, gl])).unsqueeze(0)\r\n",
        "            # Get shape of anchor box\r\n",
        "            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))\r\n",
        "            # Calculate iou between gt and anchor shapes\r\n",
        "            anch_ious = bbox_iou(gt_box, anchor_shapes)\r\n",
        "            # Where the overlap is larger than threshold set mask to zero (ignore)\r\n",
        "            conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0\r\n",
        "            # Find the best matching anchor box\r\n",
        "            best_n = np.argmax(anch_ious)\r\n",
        "            # Get ground truth box\r\n",
        "            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gl])).unsqueeze(0)\r\n",
        "            # Get the best prediction\r\n",
        "            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\r\n",
        "            # Masks\r\n",
        "            mask[b, best_n, gj, gi] = 1\r\n",
        "            conf_mask[b, best_n, gj, gi] = 1\r\n",
        "            # Coordinates\r\n",
        "            tx[b, best_n, gj, gi] = gx - gi\r\n",
        "            ty[b, best_n, gj, gi] = gy - gj\r\n",
        "            # Width and height\r\n",
        "            tw[b, best_n, gj, gi] = math.log(gw / anchors[best_n][0] + 1e-16)\r\n",
        "            tl[b, best_n, gj, gi] = math.log(gl / anchors[best_n][1] + 1e-16)\r\n",
        "            # One-hot encoding of label\r\n",
        "            target_label = int(target[b, t, 0])\r\n",
        "            tcls[b, best_n, gj, gi, target_label] = 1\r\n",
        "            tconf[b, best_n, gj, gi] = 1\r\n",
        "\r\n",
        "            # Calculate iou between ground truth and best matching prediction\r\n",
        "            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\r\n",
        "            pred_label = torch.argmax(pred_cls[b, best_n, gj, gi])\r\n",
        "            score = pred_conf[b, best_n, gj, gi]\r\n",
        "            if iou > 0.5 and pred_label == target_label and score > 0.5:\r\n",
        "                nCorrect += 1\r\n",
        "\r\n",
        "    return nGT, nCorrect, mask, conf_mask, tx, ty, tw, tl, tconf, tcls\r\n",
        "\r\n",
        "\r\n",
        "class RegionLoss(nn.Module):\r\n",
        "    def __init__(self, num_classes=8, num_anchors=5):\r\n",
        "        super(RegionLoss, self).__init__()\r\n",
        "\r\n",
        "        self.anchors = anchors\r\n",
        "        self.num_anchors = num_anchors\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.bbox_attrs = 7+num_classes\r\n",
        "        self.ignore_thres = 0.6\r\n",
        "        self.lambda_coord = 1\r\n",
        "\r\n",
        "        self.mse_loss = nn.MSELoss(size_average=True)  # Coordinate loss\r\n",
        "        self.bce_loss = nn.BCELoss(size_average=True)  # Confidence loss\r\n",
        "        self.ce_loss = nn.CrossEntropyLoss()  # Class loss\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, targets):\r\n",
        "        #x : batch_size*num_anchorsx(6+1+num_classes)*H*W    [12,75,16,32]\r\n",
        "        #targets :   targets define in utils  get_target function   [12,50,7]\r\n",
        "\r\n",
        "        nA = self.num_anchors     # num_anchors = 5\r\n",
        "        nB = x.data.size(0)  # batch_size\r\n",
        "        nH = x.data.size(2)  # nH  16\r\n",
        "        nW = x.data.size(3)  # nW  32\r\n",
        "\r\n",
        "\r\n",
        "        # Tensors for cuda support\r\n",
        "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\r\n",
        "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\r\n",
        "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\r\n",
        "\r\n",
        "        prediction = x.view(nB, nA, self.bbox_attrs, nH, nW).permute(0, 1, 3, 4, 2).contiguous()  # prediction [12,5,16,32,15]\r\n",
        "\r\n",
        "        # Get outputs\r\n",
        "        x = torch.sigmoid(prediction[..., 0])  # Center x\r\n",
        "        y = torch.sigmoid(prediction[..., 1])  # Center y\r\n",
        "        w = prediction[..., 2]  # Width\r\n",
        "        h = prediction[..., 3]  # Height\r\n",
        "        pred_conf = torch.sigmoid(prediction[..., 6])  # Conf\r\n",
        "        pred_cls = torch.sigmoid(prediction[..., 7:])  # Cls pred.\r\n",
        "\r\n",
        "        # Calculate offsets for each grid\r\n",
        "        grid_x = torch.arange(nW).repeat(nH, 1).view([1, 1, nH, nW]).type(FloatTensor)\r\n",
        "        grid_y = torch.arange(nH).repeat(nW, 1).t().view([1, 1, nH, nW]).type(FloatTensor)\r\n",
        "        scaled_anchors = FloatTensor([(a_w , a_h ) for a_w, a_h in self.anchors])\r\n",
        "        anchor_w = scaled_anchors[:, 0:1].view((1, nA, 1, 1))\r\n",
        "        anchor_h = scaled_anchors[:, 1:2].view((1, nA, 1, 1))\r\n",
        "\r\n",
        "        # Add offset and scale with anchors\r\n",
        "        pred_boxes = FloatTensor(prediction[..., :4].shape)\r\n",
        "        pred_boxes[..., 0] = x.data + grid_x\r\n",
        "        pred_boxes[..., 1] = y.data + grid_y\r\n",
        "        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\r\n",
        "        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        if x.is_cuda:\r\n",
        "            self.mse_loss = self.mse_loss.cuda()\r\n",
        "            self.bce_loss = self.bce_loss.cuda()\r\n",
        "            self.ce_loss = self.ce_loss.cuda()\r\n",
        "\r\n",
        "        nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(\r\n",
        "            pred_boxes=pred_boxes.cpu().data,\r\n",
        "            pred_conf=pred_conf.cpu().data,\r\n",
        "            pred_cls=pred_cls.cpu().data,\r\n",
        "            target=targets.cpu().data,\r\n",
        "            anchors=scaled_anchors.cpu().data,\r\n",
        "            num_anchors=nA,\r\n",
        "            num_classes=self.num_classes,\r\n",
        "            nH=nH,\r\n",
        "            nW=nW,\r\n",
        "            ignore_thres=self.ignore_thres\r\n",
        "        )\r\n",
        "\r\n",
        "        nProposals = int((pred_conf > 0.5).sum().item())\r\n",
        "        recall = float(nCorrect / nGT) if nGT else 1\r\n",
        "        precision = float(nCorrect / nProposals)\r\n",
        "\r\n",
        "        # Handle masks\r\n",
        "        mask = Variable(mask.type(ByteTensor))\r\n",
        "        conf_mask = Variable(conf_mask.type(ByteTensor))\r\n",
        "\r\n",
        "        # Handle target variables\r\n",
        "        tx = Variable(tx.type(FloatTensor), requires_grad=False)\r\n",
        "        ty = Variable(ty.type(FloatTensor), requires_grad=False)\r\n",
        "        tw = Variable(tw.type(FloatTensor), requires_grad=False)\r\n",
        "        th = Variable(th.type(FloatTensor), requires_grad=False)\r\n",
        "        tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\r\n",
        "        tcls = Variable(tcls.type(LongTensor), requires_grad=False)\r\n",
        "\r\n",
        "        # Get conf mask where gt and where there is no gt\r\n",
        "        conf_mask_true = mask\r\n",
        "        conf_mask_false = conf_mask - mask\r\n",
        "\r\n",
        "        # Mask outputs to ignore non-existing objects\r\n",
        "        loss_x = self.mse_loss(x[mask], tx[mask])\r\n",
        "        loss_y = self.mse_loss(y[mask], ty[mask])\r\n",
        "        loss_w = self.mse_loss(w[mask], tw[mask])\r\n",
        "        loss_h = self.mse_loss(h[mask], th[mask])\r\n",
        "        loss_conf = self.bce_loss(pred_conf[conf_mask_false], tconf[conf_mask_false]) + self.bce_loss(\r\n",
        "            pred_conf[conf_mask_true], tconf[conf_mask_true]\r\n",
        "        )\r\n",
        "        loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))\r\n",
        "        loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\r\n",
        "\r\n",
        "        #print('nGT %d, recall %f, precision %f, proposals %d, loss: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f' % \\\r\n",
        "         #        (nGT, recall,  precision,  nProposals, loss_x.data, loss_y.data, loss_w.data, loss_h.data, loss_conf.data, loss_cls.data,loss.data))\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLMvbDzFLUyV"
      },
      "source": [
        "# Complex-YOLO\r\n",
        "Model with 18 convolution layers and 5 MaxPool layers, each layer has a Batch Normalization followed by A leaky RELu except the last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9zKITMbLWXC"
      },
      "source": [
        "def reorg(x):\r\n",
        "    stride = 2\r\n",
        "    assert(x.data.dim() == 4)\r\n",
        "    B = x.data.size(0)\r\n",
        "    C = x.data.size(1)\r\n",
        "    H = x.data.size(2)\r\n",
        "    W = x.data.size(3)\r\n",
        "    assert(H % stride == 0)\r\n",
        "    assert(W % stride == 0)\r\n",
        "    ws = stride\r\n",
        "    hs = stride\r\n",
        "    x = x.view(B, C, int(H/hs), hs, int(W/ws), ws).transpose(3,4).contiguous()\r\n",
        "    x = x.view(B, C, int(H/hs*W/ws), hs*ws).transpose(2,3).contiguous()\r\n",
        "    x = x.view(B, C, hs*ws, int(H/hs), int(W/ws)).transpose(1,2).contiguous()\r\n",
        "    x = x.view(B, hs*ws*C, int(H/hs), int(W/ws))\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class ComplexYOLO(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ComplexYOLO, self).__init__()\r\n",
        "        self.conv_1 = nn.Conv2d(in_channels=3,out_channels=24,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_1   = nn.BatchNorm2d(num_features=24)\r\n",
        "        self.pool_1 = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "        self.conv_2 = nn.Conv2d(in_channels=24,out_channels=48,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_2   = nn.BatchNorm2d(num_features=48)\r\n",
        "        self.pool_2 = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "        self.conv_3 = nn.Conv2d(in_channels=48,out_channels=64,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_3   = nn.BatchNorm2d(num_features=64)\r\n",
        "        self.conv_4 = nn.Conv2d(in_channels=64,out_channels=32,kernel_size=1,stride=1,padding=0)\r\n",
        "        self.bn_4   = nn.BatchNorm2d(num_features=32)\r\n",
        "        self.conv_5 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_5   = nn.BatchNorm2d(num_features=64)\r\n",
        "        self.pool_3 = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "        self.conv_6 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_6   = nn.BatchNorm2d(num_features=128)\r\n",
        "        self.conv_7 = nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_7   = nn.BatchNorm2d(num_features=64)\r\n",
        "        self.conv_8 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_8   = nn.BatchNorm2d(num_features=128)\r\n",
        "        self.pool_4 = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "        self.conv_9  = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_9    = nn.BatchNorm2d(num_features=256)\r\n",
        "        self.conv_10 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=1,stride=1,padding=0)\r\n",
        "        self.bn_10   = nn.BatchNorm2d(num_features=256)\r\n",
        "        self.conv_11 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_11   = nn.BatchNorm2d(num_features=512)\r\n",
        "        self.pool_5  = nn.MaxPool2d(2)\r\n",
        "\r\n",
        "        self.conv_12 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_12   = nn.BatchNorm2d(num_features=512)\r\n",
        "        self.conv_13 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=1,stride=1,padding=0)\r\n",
        "        self.bn_13   = nn.BatchNorm2d(num_features=512)\r\n",
        "        self.conv_14 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_14   = nn.BatchNorm2d(num_features=1024)\r\n",
        "        self.conv_15 = nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_15   = nn.BatchNorm2d(num_features=1024)\r\n",
        "        self.conv_16 = nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_16   = nn.BatchNorm2d(num_features=1024)\r\n",
        "\r\n",
        "        self.conv_17 = nn.Conv2d(in_channels=2048,out_channels=1024,kernel_size=3,stride=1,padding=1)\r\n",
        "        self.bn_17   = nn.BatchNorm2d(num_features=1024)\r\n",
        "        self.conv_18 = nn.Conv2d(in_channels=1024,out_channels=75,kernel_size=1,stride=1,padding=0)\r\n",
        "\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        x = self.relu(self.bn_1(self.conv_1(x)))\r\n",
        "        x = self.pool_1(x)\r\n",
        "        \r\n",
        "        x = self.relu(self.bn_2(self.conv_2(x)))\r\n",
        "        x = self.pool_2(x)\r\n",
        "\r\n",
        "        x = self.relu(self.bn_3(self.conv_3(x)))\r\n",
        "        x = self.relu(self.bn_4(self.conv_4(x)))\r\n",
        "        x = self.relu(self.bn_5(self.conv_5(x)))\r\n",
        "        x = self.pool_3(x)\r\n",
        "\r\n",
        "        x = self.relu(self.bn_6(self.conv_6(x)))\r\n",
        "        x = self.relu(self.bn_7(self.conv_7(x)))\r\n",
        "        x = self.relu(self.bn_8(self.conv_8(x)))\r\n",
        "        x = self.pool_4(x)\r\n",
        "        \r\n",
        "        x = self.relu(self.bn_9(self.conv_9(x)))\r\n",
        "        route_1 = x            # 12 layer\r\n",
        "        reorg_result = reorg(route_1)\r\n",
        "        \r\n",
        "        x = self.relu(self.bn_10(self.conv_10(x)))\r\n",
        "        x = self.relu(self.bn_11(self.conv_11(x)))\r\n",
        "        x = self.pool_5(x)\r\n",
        "\r\n",
        "        x = self.relu(self.bn_12(self.conv_12(x)))\r\n",
        "        x = self.relu(self.bn_13(self.conv_13(x)))\r\n",
        "        x = self.relu(self.bn_14(self.conv_14(x)))\r\n",
        "        x = self.relu(self.bn_15(self.conv_15(x)))\r\n",
        "        x = self.relu(self.bn_16(self.conv_16(x)))\r\n",
        "\r\n",
        "        x = torch.cat((reorg_result,x),1)\r\n",
        "        x = self.relu(self.bn_17(self.conv_17(x)))\r\n",
        "        x = self.conv_18(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_zQxQARATpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02443b9e-4fde-4e49-f1ad-cc5e2d71200d"
      },
      "source": [
        "model=ComplexYOLO()\r\n",
        "model.cuda()\r\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComplexYOLO(\n",
              "  (conv_1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_2): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_4): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (bn_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_7): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (bn_10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_13): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (bn_13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_14): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_15): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_15): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_16): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_16): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_17): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn_17): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_18): Conv2d(1024, 75, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (relu): ReLU(inplace=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5y-u0IOV6CR"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A_Z_LWuLAWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be3b0a8-5ab9-4589-a88e-a310363c3bde"
      },
      "source": [
        "\r\n",
        "import time\r\n",
        "batch_size=32\r\n",
        "\r\n",
        "# dataset\r\n",
        "dataset=KittiDataset(root='/content/drive/MyDrive/dlproject',set='train')\r\n",
        "data_loader = data.DataLoader(dataset, batch_size, shuffle=True,pin_memory=True,num_workers=4)\r\n",
        "\r\n",
        "model = ComplexYOLO()\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "# define optimizer\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "\r\n",
        "# define loss function\r\n",
        "region_loss = RegionLoss(num_classes=8, num_anchors=5)\r\n",
        "\r\n",
        "lossl=[]\r\n",
        "c=0\r\n",
        "\r\n",
        "for epoch in range(101):\r\n",
        "   start=time.time()\r\n",
        "   for batch_idx, (rgb_map, target) in enumerate(data_loader):          \r\n",
        "          optimizer.zero_grad()\r\n",
        "\r\n",
        "          rgb_map = rgb_map.view(rgb_map.data.size(0),rgb_map.data.size(3),rgb_map.data.size(1),rgb_map.data.size(2))\r\n",
        "          output = model(rgb_map.float().cuda())\r\n",
        "\r\n",
        "          loss = region_loss(output,target)\r\n",
        "          \r\n",
        "\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "\r\n",
        "   if (epoch % 2 == 0):\r\n",
        "       torch.save(model, \"ComplexYOLO_epoch\"+str(epoch))\r\n",
        "   c=c+1\r\n",
        "   print(c)\r\n",
        "   lossl.append(loss)\r\n",
        "   end=time.time()\r\n",
        "   print(\"Time for epoch {} is {} with loss {}\".format(epoch+1,(end-start/60.0),loss))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:173: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:174: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:175: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:176: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:177: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:180: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "Time for epoch 0 is 1580981220.0650406 with loss 1.136518120765686\n",
            "2\n",
            "Time for epoch 1 is 1580981632.5840263 with loss 0.9074435234069824\n",
            "3\n",
            "Time for epoch 2 is 1580982046.7056108 with loss 0.8614804744720459\n",
            "4\n",
            "Time for epoch 3 is 1580982465.046965 with loss 0.8973052501678467\n",
            "5\n",
            "Time for epoch 4 is 1580982887.1141968 with loss 0.9465278387069702\n",
            "6\n",
            "Time for epoch 5 is 1580983299.6566477 with loss 0.6788968443870544\n",
            "7\n",
            "Time for epoch 6 is 1580983713.9436116 with loss 1.285749912261963\n",
            "8\n",
            "Time for epoch 7 is 1580984134.142567 with loss 0.7804110050201416\n",
            "9\n",
            "Time for epoch 8 is 1580984560.9258692 with loss 0.7364298105239868\n",
            "10\n",
            "Time for epoch 9 is 1580984982.462145 with loss 0.5182375907897949\n",
            "11\n",
            "Time for epoch 10 is 1580985410.137557 with loss 0.8805631399154663\n",
            "12\n",
            "Time for epoch 11 is 1580985828.0143454 with loss 0.6175538897514343\n",
            "13\n",
            "Time for epoch 12 is 1580986247.755204 with loss 0.8127908110618591\n",
            "14\n",
            "Time for epoch 13 is 1580986661.903984 with loss 0.4443184435367584\n",
            "15\n",
            "Time for epoch 14 is 1580987079.0513053 with loss 0.6809097528457642\n",
            "16\n",
            "Time for epoch 15 is 1580987489.5695968 with loss 0.48260435461997986\n",
            "17\n",
            "Time for epoch 16 is 1580987908.4084342 with loss 0.4787712097167969\n",
            "18\n",
            "Time for epoch 17 is 1580988321.6286218 with loss 0.48306185007095337\n",
            "19\n",
            "Time for epoch 18 is 1580988734.255913 with loss 0.39752575755119324\n",
            "20\n",
            "Time for epoch 19 is 1580989145.2156425 with loss 0.3508298993110657\n",
            "21\n",
            "Time for epoch 20 is 1580989557.2078443 with loss 0.35276132822036743\n",
            "22\n",
            "Time for epoch 21 is 1580989968.5510483 with loss 0.3587993383407593\n",
            "23\n",
            "Time for epoch 22 is 1580990382.2644968 with loss 0.3788391947746277\n",
            "24\n",
            "Time for epoch 23 is 1580990797.2836099 with loss 0.31694209575653076\n",
            "25\n",
            "Time for epoch 24 is 1580991210.64561 with loss 0.3004545271396637\n",
            "26\n",
            "Time for epoch 25 is 1580991635.0102468 with loss 0.304178386926651\n",
            "27\n",
            "Time for epoch 26 is 1580992053.3036973 with loss 0.24816402792930603\n",
            "28\n",
            "Time for epoch 27 is 1580992468.9743817 with loss 0.2389012724161148\n",
            "29\n",
            "Time for epoch 28 is 1580992885.6197739 with loss 0.23498837649822235\n",
            "30\n",
            "Time for epoch 29 is 1580993298.8568497 with loss 0.23123356699943542\n",
            "31\n",
            "Time for epoch 30 is 1580993710.7428591 with loss 0.21181076765060425\n",
            "32\n",
            "Time for epoch 31 is 1580994123.2615564 with loss 0.22639527916908264\n",
            "33\n",
            "Time for epoch 32 is 1580994537.806813 with loss 0.23027190566062927\n",
            "34\n",
            "Time for epoch 33 is 1580994946.6263783 with loss 0.19081352651119232\n",
            "35\n",
            "Time for epoch 34 is 1580995364.150882 with loss 0.17943085730075836\n",
            "36\n",
            "Time for epoch 35 is 1580995781.099879 with loss 0.17266207933425903\n",
            "37\n",
            "Time for epoch 36 is 1580996192.7100399 with loss 0.17771315574645996\n",
            "38\n",
            "Time for epoch 37 is 1580996602.3889964 with loss 0.19360271096229553\n",
            "39\n",
            "Time for epoch 38 is 1580997010.852274 with loss 0.17098504304885864\n",
            "40\n",
            "Time for epoch 39 is 1580997423.6956131 with loss 0.15967386960983276\n",
            "41\n",
            "Time for epoch 40 is 1580997833.1277347 with loss 0.17040158808231354\n",
            "42\n",
            "Time for epoch 41 is 1580998245.379921 with loss 0.2684840261936188\n",
            "43\n",
            "Time for epoch 42 is 1580998656.0896807 with loss 0.17131635546684265\n",
            "44\n",
            "Time for epoch 43 is 1580999073.6851978 with loss 0.14079874753952026\n",
            "45\n",
            "Time for epoch 44 is 1580999484.7057931 with loss 0.14411212503910065\n",
            "46\n",
            "Time for epoch 45 is 1580999892.0945032 with loss 0.12097017467021942\n",
            "47\n",
            "Time for epoch 46 is 1581000298.4719293 with loss 0.12336385995149612\n",
            "48\n",
            "Time for epoch 47 is 1581000709.3493435 with loss 0.11000587046146393\n",
            "49\n",
            "Time for epoch 48 is 1581001119.7872322 with loss 0.1082848459482193\n",
            "50\n",
            "Time for epoch 49 is 1581001527.5644944 with loss 0.10670538991689682\n",
            "51\n",
            "Time for epoch 50 is 1581001938.7672086 with loss 0.11355464905500412\n",
            "52\n",
            "Time for epoch 51 is 1581002356.8286011 with loss 0.2303694486618042\n",
            "53\n",
            "Time for epoch 52 is 1581002771.845541 with loss 0.1885286271572113\n",
            "54\n",
            "Time for epoch 53 is 1581003179.9139416 with loss 0.150878444314003\n",
            "55\n",
            "Time for epoch 54 is 1581003591.4461906 with loss 0.11926841735839844\n",
            "56\n",
            "Time for epoch 55 is 1581004003.2951682 with loss 0.10457535088062286\n",
            "57\n",
            "Time for epoch 56 is 1581004414.8849761 with loss 0.10710079967975616\n",
            "58\n",
            "Time for epoch 57 is 1581004826.6102698 with loss 0.10922878235578537\n",
            "59\n",
            "Time for epoch 58 is 1581005241.3356898 with loss 0.10018622875213623\n",
            "60\n",
            "Time for epoch 59 is 1581005654.8692846 with loss 0.10009931027889252\n",
            "61\n",
            "Time for epoch 60 is 1581006066.8510828 with loss 0.09745736420154572\n",
            "62\n",
            "Time for epoch 61 is 1581006477.0507298 with loss 0.09663472324609756\n",
            "63\n",
            "Time for epoch 62 is 1581006890.703636 with loss 0.09399639070034027\n",
            "64\n",
            "Time for epoch 63 is 1581007304.1486423 with loss 0.11322058737277985\n",
            "65\n",
            "Time for epoch 64 is 1581007719.3524206 with loss 0.26103562116622925\n",
            "66\n",
            "Time for epoch 65 is 1581008159.7596798 with loss 0.1565377414226532\n",
            "67\n",
            "Time for epoch 66 is 1581008576.1934688 with loss 0.121675506234169\n",
            "68\n",
            "Time for epoch 67 is 1581008991.854577 with loss 0.11365092545747757\n",
            "69\n",
            "Time for epoch 68 is 1581009409.7904158 with loss 0.09918451309204102\n",
            "70\n",
            "Time for epoch 69 is 1581009823.3464696 with loss 0.09736628830432892\n",
            "71\n",
            "Time for epoch 70 is 1581010241.9256415 with loss 0.09392955899238586\n",
            "72\n",
            "Time for epoch 71 is 1581010680.0578349 with loss 0.09428076446056366\n",
            "73\n",
            "Time for epoch 72 is 1581011095.2047598 with loss 0.08754559606313705\n",
            "74\n",
            "Time for epoch 73 is 1581011506.6640706 with loss 0.0930265262722969\n",
            "75\n",
            "Time for epoch 74 is 1581011922.6134434 with loss 0.09243180602788925\n",
            "76\n",
            "Time for epoch 75 is 1581012336.396579 with loss 0.09213665127754211\n",
            "77\n",
            "Time for epoch 76 is 1581012753.6725137 with loss 0.09417374432086945\n",
            "78\n",
            "Time for epoch 77 is 1581013167.6435359 with loss 0.09802258014678955\n",
            "79\n",
            "Time for epoch 78 is 1581013593.3543282 with loss 0.5022832155227661\n",
            "80\n",
            "Time for epoch 79 is 1581014008.567219 with loss 0.14810039103031158\n",
            "81\n",
            "Time for epoch 80 is 1581014423.7255352 with loss 0.12727417051792145\n",
            "82\n",
            "Time for epoch 81 is 1581014839.4036372 with loss 0.10223115980625153\n",
            "83\n",
            "Time for epoch 82 is 1581015254.3887184 with loss 0.09578187018632889\n",
            "84\n",
            "Time for epoch 83 is 1581015669.4135387 with loss 0.09522293508052826\n",
            "85\n",
            "Time for epoch 84 is 1581016084.993441 with loss 0.08776653558015823\n",
            "86\n",
            "Time for epoch 85 is 1581016502.0380003 with loss 0.08881666511297226\n",
            "87\n",
            "Time for epoch 86 is 1581016916.0764303 with loss 0.09972751140594482\n",
            "88\n",
            "Time for epoch 87 is 1581017325.6592736 with loss 0.09247976541519165\n",
            "89\n",
            "Time for epoch 88 is 1581017743.2069185 with loss 0.09548349678516388\n",
            "90\n",
            "Time for epoch 89 is 1581018158.5886335 with loss 0.09624829143285751\n",
            "91\n",
            "Time for epoch 90 is 1581018577.7125947 with loss 0.09175735712051392\n",
            "92\n",
            "Time for epoch 91 is 1581018990.5092044 with loss 0.08933086693286896\n",
            "93\n",
            "Time for epoch 92 is 1581019405.8048682 with loss 0.09184416383504868\n",
            "94\n",
            "Time for epoch 93 is 1581019819.4362295 with loss 0.10201045870780945\n",
            "95\n",
            "Time for epoch 94 is 1581020233.407166 with loss 0.1035606637597084\n",
            "96\n",
            "Time for epoch 95 is 1581020646.7162929 with loss 0.34129106998443604\n",
            "97\n",
            "Time for epoch 96 is 1581021059.1039379 with loss 0.14970700442790985\n",
            "98\n",
            "Time for epoch 97 is 1581021475.5607717 with loss 0.1315600872039795\n",
            "99\n",
            "Time for epoch 98 is 1581021893.1522398 with loss 0.11030305922031403\n",
            "100\n",
            "Time for epoch 99 is 1581022305.5333292 with loss 0.09923838824033737\n",
            "101\n",
            "Time for epoch 100 is 1581022723.8974829 with loss 0.08986161649227142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTjSRfrRvm02"
      },
      "source": [
        "for l in range(len(lossl)):\r\n",
        "  lossl[l]=lossl[l].cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Szpn-6Q3rw"
      },
      "source": [
        "Plot of Loss vs Epochs\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2vQ4VXIiu4ZD",
        "outputId": "f163bccb-16e6-479f-e827-5ad81625ccf6"
      },
      "source": [
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"loss\")\r\n",
        "plt.plot(lossl)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH-ArSzEyDsy"
      },
      "source": [
        "As we can see that because of the dataset, we have an overfitting problem hence we use the early stopping technique and take the model saved from the 62nd epoch\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBdx96LgV9PT"
      },
      "source": [
        "# Eval\r\n",
        "function to get and draw the predicted classification bounding boxes along with the appropriate labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAo4mBPmV-rp"
      },
      "source": [
        "from progressbar import ProgressBar\r\n",
        "pbar=ProgressBar()\r\n",
        "import imageio\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "def drawRect(img, pt1, pt2, pt3, pt4, color, lineWidth):\r\n",
        "    cv2.line(img, pt1, pt2, color, lineWidth)\r\n",
        "    cv2.line(img, pt2, pt3, color, lineWidth)\r\n",
        "    cv2.line(img, pt3, pt4, color, lineWidth)\r\n",
        "    cv2.line(img, pt1, pt4, color, lineWidth)\r\n",
        "\r\n",
        "\r\n",
        "def get_region_boxes(x, conf_thresh, num_classes, anchors, num_anchors):\r\n",
        "    if x.dim() == 3:\r\n",
        "        x = x.unsqueeze(0)\r\n",
        "\r\n",
        "    assert (x.size(1) == (7 + num_classes) * num_anchors)\r\n",
        "\r\n",
        "    nA = num_anchors  # num_anchors = 5\r\n",
        "    nB = x.data.size(0)\r\n",
        "    nC = num_classes  # num_classes = 8\r\n",
        "    nH = x.data.size(2)  # nH  16\r\n",
        "    nW = x.data.size(3)  # nW  32\r\n",
        "\r\n",
        "    # Tensors for cuda support\r\n",
        "    FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\r\n",
        "    LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\r\n",
        "    ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\r\n",
        "\r\n",
        "    prediction = x.view(nB, nA, 7+num_classes, nH, nW).permute(0, 1, 3, 4, 2).contiguous()\r\n",
        "\r\n",
        "    # Get outputs\r\n",
        "    x = torch.sigmoid(prediction[..., 0])  # Center x\r\n",
        "    y = torch.sigmoid(prediction[..., 1])  # Center y\r\n",
        "    w = prediction[..., 2]  # Width\r\n",
        "    h = prediction[..., 3]  # Height\r\n",
        "    pred_conf = torch.sigmoid(prediction[..., 6])  # Conf\r\n",
        "    pred_cls = torch.sigmoid(prediction[..., 7:])  # Cls pred.\r\n",
        "\r\n",
        "    # Calculate offsets for each grid\r\n",
        "    grid_x = torch.arange(nW).repeat(nH, 1).view([1, 1, nH, nW]).type(FloatTensor)\r\n",
        "    grid_y = torch.arange(nH).repeat(nW, 1).t().view([1, 1, nH, nW]).type(FloatTensor)\r\n",
        "    scaled_anchors = FloatTensor([(a_w , a_h ) for a_w, a_h in anchors])\r\n",
        "    anchor_w = scaled_anchors[:, 0:1].view((1, nA, 1, 1))\r\n",
        "    anchor_h = scaled_anchors[:, 1:2].view((1, nA, 1, 1))\r\n",
        "\r\n",
        "    # Add offset and scale with anchors\r\n",
        "    pred_boxes = FloatTensor(prediction.shape)\r\n",
        "    pred_boxes[..., 0] = x.data + grid_x\r\n",
        "    pred_boxes[..., 1] = y.data + grid_y\r\n",
        "    pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\r\n",
        "    pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\r\n",
        "\r\n",
        "    pred_boxes[..., 6] = pred_conf\r\n",
        "    pred_boxes[..., 7:(7 + nC) ] = pred_cls\r\n",
        "\r\n",
        "    pred_boxes = convert2cpu(pred_boxes.transpose(0, 1).contiguous().view(-1, (7 + nC)))  # torch.Size([2560, 15])\r\n",
        "    \r\n",
        "    all_boxes = []\r\n",
        "    for i in range(2560):\r\n",
        "        cls=torch.argmax(pred_boxes[i][7:15])\r\n",
        "        if cls==0:\r\n",
        "            conf_thresh=0.7\r\n",
        "        elif cls==3 or cls==5:\r\n",
        "            conf_thresh=0.5\r\n",
        "        else:\r\n",
        "            conf_thresh=0.5\r\n",
        "        #print(\"conf_thresh=\",conf_thresh)    \r\n",
        "        if pred_boxes[i][6] > conf_thresh:\r\n",
        "            all_boxes.append(pred_boxes[i])\r\n",
        "            # print(pred_boxes[i])\r\n",
        "    return all_boxes\r\n",
        "\r\n",
        "\r\n",
        "# classes\r\n",
        "# class_list = ['Car', 'Van' , 'Truck' , 'Pedestrian' , 'Person_sitting' , 'Cyclist' , 'Tram' ]\r\n",
        "colour_list = [(255,0,0),(0,255,0),(0,0,255),(255,255,0),(0,255,255),(255,0,255),(0,255,200)]\r\n",
        "\r\n",
        "\r\n",
        "bc = {}\r\n",
        "bc['minX'] = 0;\r\n",
        "bc['maxX'] = 80;\r\n",
        "bc['minY'] = -40;\r\n",
        "bc['maxY'] = 40\r\n",
        "bc['minZ'] = -2;\r\n",
        "bc['maxZ'] = 1.25\r\n",
        "region_loss = RegionLoss(num_classes=8, num_anchors=5)\r\n",
        "\r\n",
        "#This is for evaluatiing images of particular range\r\n",
        "for file_i in pbar(range(6030,6230)):\r\n",
        "    #print(\"epoch=\",file_i)\r\n",
        "    start=time.time()\r\n",
        "    test_i = str(file_i).zfill(6)\r\n",
        "\r\n",
        "    lidar_file = '/content/drive/MyDrive/dlproject/training/velodyne/' + test_i + '.bin'\r\n",
        "    calib_file = '/content/drive/MyDrive/dlproject/training/calib/' + test_i + \".txt\"\r\n",
        "    label_file = '/content/drive/MyDrive/dlproject/training/label_2/' + test_i + \".txt\"\r\n",
        "#for running on the testing images that do not have labels\r\n",
        "#    lidar_file = '/content/drive/MyDrive/dlproject/testing/velodyne/' + test_i + '.bin'\r\n",
        "#    calib_file = '/content/drive/MyDrive/dlproject/testing/calib/' + test_i + \".txt\"\r\n",
        "\r\n",
        "\r\n",
        "    # load target data\r\n",
        "    calib = load_kitti_calib(calib_file)\r\n",
        "    #comment the line below if using testing dataset as there is no target\r\n",
        "    target = get_target(label_file, calib['Tr_velo2cam'])\r\n",
        "    #print(target)\r\n",
        "\r\n",
        "    # load point cloud data\r\n",
        "    a = np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 4)\r\n",
        "    b = removePoints(a, bc)\r\n",
        "    rgb_map = makeBVFeature(b, bc, 40 / 512)\r\n",
        "   \r\n",
        "    imageio.imwrite('/content/drive/MyDrive/dlproject/training/results/eval_bv.png', (rgb_map*255).astype(np.uint8))\r\n",
        "\r\n",
        "    # load trained model  and  forward\r\n",
        "    input = torch.from_numpy(rgb_map)  # (512, 1024, 3)\r\n",
        "    input = input.reshape(1, 3, 512, 1024)\r\n",
        "    model = torch.load('/content/drive/MyDrive/dlproject/training/ComplexYOLO_epoch62')\r\n",
        "    model.cuda()\r\n",
        "    output = model(input.float().cuda())  # torch.Size([1, 75, 16, 32])\r\n",
        "   \r\n",
        "\r\n",
        "\r\n",
        "    # eval result\r\n",
        "    conf_thresh = 0.7\r\n",
        "  \r\n",
        "    num_classes = int(8)\r\n",
        "    num_anchors = int(5)\r\n",
        "    img = cv2.imread('/content/drive/MyDrive/dlproject/training/results/eval_bv.png')\r\n",
        "\r\n",
        "    all_boxes = get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors)\r\n",
        "\r\n",
        "    for i in range(len(all_boxes)):\r\n",
        "        pred_img_y = int(all_boxes[i][0] * 1024.0 / 32.0)  # 32 cell = 1024 pixels\r\n",
        "        pred_img_x = int(all_boxes[i][1] * 512.0 / 16.0)  # 16 cell = 512 pixels\r\n",
        "        pred_img_width = int(all_boxes[i][2] * 1024.0 / 32.0)  # 32 cell = 1024 pixels\r\n",
        "        pred_img_height = int(all_boxes[i][3] * 512.0 / 16.0)  # 16 cell = 512 pixels\r\n",
        "\r\n",
        "        rect_top1 = int(pred_img_y - pred_img_width / 2)\r\n",
        "        rect_top2 = int(pred_img_x - pred_img_height / 2)\r\n",
        "        rect_bottom1 = int(pred_img_y + pred_img_width / 2)\r\n",
        "        rect_bottom2 = int(pred_img_x + pred_img_height / 2)\r\n",
        "        \r\n",
        "        index=torch.argmax(all_boxes[i][7:15])\r\n",
        "        #drawing classifciation boxes with appropriate labels\r\n",
        "        cv2.rectangle(img, (rect_top1, rect_top2), (rect_bottom1, rect_bottom2),colour_list[torch.argmax(all_boxes[i][7:15])] ,1)\r\n",
        "        (test_width, text_height), baseline = cv2.getTextSize(class_list[index],cv2.FONT_HERSHEY_PLAIN,0.75, 1)\r\n",
        "        cv2.rectangle(img, (rect_top1, rect_top2), (rect_top1+test_width, rect_top2-text_height-baseline),colour_list[index] ,thickness=cv2.FILLED)\r\n",
        "        cv2.putText(img, class_list[index], (rect_top1, rect_top2 - baseline), cv2.FONT_HERSHEY_PLAIN, 0.75, (0, 0, 0), 1)\r\n",
        "\r\n",
        "\r\n",
        "    #code for generating the actual bounding boxes if using training data\r\n",
        "    # for j in range(50):\r\n",
        "    #      if target[j][1] == 0:\r\n",
        "    #          break\r\n",
        "    #      img_y = int(target[j][1] * 1024.0)  # 32 cell = 1024 pixels\r\n",
        "    #      img_x = int(target[j][2] * 512.0)  # 16 cell = 512 pixels\r\n",
        "    #      img_width = int(target[j][3] * 1024.0)  # 32 cell = 1024 pixels\r\n",
        "    #      img_height = int(target[j][4] * 512.0)  # 16 cell = 512 pixels\r\n",
        "    \r\n",
        "    #      rect_top1 = int(img_y - img_width / 2)\r\n",
        "    #      rect_top2 = int(img_x - img_height / 2)\r\n",
        "    #      rect_bottom1 = int(img_y + img_width / 2)\r\n",
        "    #      rect_bottom2 = int(img_x + img_height / 2)\r\n",
        "    #      cv2.rectangle(img, (rect_top1, rect_top2), (rect_bottom1, rect_bottom2), (255 ,255, 255), 1)\r\n",
        "\r\n",
        "    \r\n",
        "    #cv2_imshow(img)\r\n",
        "    imageio.imwrite('/content/drive/MyDrive/dlproject/training/label_3/eval_bv' + test_i + '.png', img)\r\n",
        "    end=time.time()\r\n",
        "    #print(\"time taken in this epoch=\",(end-start)/60.0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}